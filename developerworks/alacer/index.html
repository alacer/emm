<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>R and Streams: Analyzing Streaming Data Using R</title><meta http-equiv="PICS-Label" content="(PICS-1.1 &quot;http://www.icra.org/ratingsv02.html&quot; l gen true r (cz 1 lz 1 nz 1 oz 1 vz 1) &quot;http://www.rsac.org/ratingsv01.html&quot; l gen true r (n 0 s 0 v 0 l 0) &quot;http://www.classify.org/safesurf/&quot; l gen true r (SS~~000 1))" /><link rel="schema.DC" href="http://purl.org/DC/elements/1.0/" /><link rel="SHORTCUT ICON" href="http://www.ibm.com/favicon.ico" /><meta name="Owner" content="dW Information/Raleigh/IBM" /><meta name="DC.Language" scheme="rfc1766" content="en" /><meta name="IBM.Country" content="ZZ" /><meta name="Security" content="Public" /><meta name="IBM.SpecialPurpose" content="SP001" /><meta name="IBM.PageAttributes" content="sid=1003" /><meta name="Source" content="v16 Template Generator" /><meta name="Robots" content="index,follow" /><meta name="Abstract" content="Web server logs can be used to study the browsing habits of users. For example, in [1], the authors use mixtures of first-order Markov models to discover clusters of Web pages on a web site. They use these models to predict which pages users are likely to request next. The authors propose using this information to “pre-fetch” a page before a user actually requests it. In this work, we perform a similar analysis of web server logs to predict how users browse a website. We perform the analysis in real-time using IBM Infosphere Streams combined with R. To perform the stream clustering we use a recent implementation of the extensible Markov model called rEMM [2]."></meta><meta name="Description" content="Web server logs can be used to study the browsing habits of users. For example, in [1], the authors use mixtures of first-order Markov models to discover clusters of Web pages on a web site. They use these models to predict which pages users are likely to request next. The authors propose using this information to “pre-fetch” a page before a user actually requests it. In this work, we perform a similar analysis of web server logs to predict how users browse a website. We perform the analysis in real-time using IBM Infosphere Streams combined with R. To perform the stream clustering we use a recent implementation of the extensible Markov model called rEMM [2]."></meta><meta name="Keywords" content=", tttjca, tttosca"></meta><meta name="DC.Date" scheme="iso8601" content="2010-12-01"></meta><meta name="DC.Type" scheme="IBM_ContentClassTaxonomy" content="CT316"></meta><meta name="DC.Subject" scheme="IBM_SubjectTaxonomy" content="TT300"></meta><meta scheme="IBM_WTMCategory" name="IBM.WTMCategory" content="SOFDCJVAZZ" /><meta name="DC.Rights" content="© Copyright IBM Corporation 2010"></meta><meta name="IBM.Effective" scheme="W3CDTF" content="2010-12-01"></meta><meta name="title" content="R and Streams: Analyzing Streaming Data Using R"></meta><!-- HEADER_SCRIPTS_AND_CSS_INCLUDE --><link href="http://dw1.s81c.com/common/v16/css/all.css" media="all" rel="stylesheet" title="www" type="text/css" /><link href="http://dw1.s81c.com/common/v16/css/screen.css" media="screen,projection" rel="stylesheet" title="www" type="text/css" /><link href="http://dw1.s81c.com/common/v16/css/screen-uas.css" media="screen,projection" rel="stylesheet" title="www" type="text/css" /><link href="http://dw1.s81c.com/common/v16/css/zz/en/screen-fonts.css" media="screen,projection" rel="stylesheet" title="www" type="text/css" /><link href="http://dw1.s81c.com/common/v16/css/handheld.css" media="handheld" rel="stylesheet" title="www" type="text/css" /><link href="http://dw1.s81c.com/common/v16/css/print.css" media="print" rel="stylesheet" title="www" type="text/css" /><!-- dW-specific CSS --><link href="http://dw1.s81c.com/developerworks/css/dw-screen.css" media="screen,projection" rel="stylesheet" title="www" type="text/css" /><link href="http://dw1.s81c.com/developerworks/js/jquery/cluetip98/jquery.cluetip.css" media="screen,projection" rel="stylesheet" title="www" type="text/css" /><link href="http://dw1.s81c.com/developerworks/css/dw-mf/dw-mf.css" rel="stylesheet" title="www" type="text/css" /><link href="http://dw1.s81c.com/developerworks/css/dw-mf/dw-mf-minimal.css" rel="stylesheet" title="www" type="text/css" /><script src="http://dw1.s81c.com/common/js/ibmcommon.js" type="text/javascript">//</script><!-- dW functional JS --><script language="JavaScript" src="http://dw1.s81c.com/developerworks/js/urltactic.js" type="text/javascript">//</script><!-- Rating_START --><script language="JavaScript" src="http://dw1.s81c.com/developerworks/js/artrating/artrating.js" type="text/javascript">//</script><style type="text/css">
.metavalue {
  display: none;
}
</style><!-- Rating_END --><!-- RESERVED_HEADER_INCLUDE --><script language="javascript" src="http://dw1.s81c.com/developerworks/js/ajax1.js" type="text/javascript">//</script><script language="javascript" src="http://dw1.s81c.com/developerworks/js/search_counter-maverick.js" type="text/javascript">//</script><script language="javascript" src="http://dw1.s81c.com/developerworks/js/request_referer_capture-maverick.js" type="text/javascript">//</script><script language="JavaScript" type="text/javascript"><!--
 setDefaultQuery('');
 //--></script><script language="JavaScript" type="text/javascript"><!--
 function openNewWindow(url,tar,arg){window.open(url,tar,arg);}
 //--></script></head><body id="ibm-com"><div id="ibm-top" class="ibm-landing-page"><!-- MASTHEAD_BEGIN --><div class="ibm-access"><a href="#ibm-content">Skip to main content</a></div><div id="ibm-masthead-dw"><div id="dw-masthead-top-row"><ul id="ibm-unav-home-dw"><li id="ibm-logo"><a href="http://www.ibm.com/us/en/"><img src="http://dw1.s81c.com/developerworks/i/mf/ibm-smlogo.gif" width="44" height="16" alt="IBM®" /></a></li></ul></div><div id="ibm-universal-nav-dw"><img src="http://dw1.s81c.com/developerworks/i/mf/dw-mast-orange-slim.jpg" width="930" height="75" alt="developerWorks®" /></div></div><!-- MASTHEAD_END --><div id="ibm-pcon"><!-- CONTENT_BEGIN --><div id="ibm-content"><!-- Navigation_Trail_BEGIN --><!----><div id="ibm-content-head" xmlns=""><ul id="ibm-navigation-trail"><li class="ibm-first"><a href="http://www.ibm.com/developerworks/">developerWorks</a></li><li><a href="http://www.ibm.com/developerworks/topics/">Technical topics</a></li><li><a href="http://www.ibm.com/developerworks/java/">Java technology</a></li><li><a href="http://www.ibm.com/developerworks/java/library/">Technical library</a></li></ul></div><!-- Navigation_Trail_END --><!-- dW_Summary Area_START --><div id="dw-summary-article"><div class="dw-content-head"><h1 xmlns="">R and Streams: Analyzing Streaming Data Using R</h1><p xmlns="" /></div><div class="ibm-container-body ibm-two-column"><div class="ibm-column ibm-first"><div class="author" xmlns="">David C. Hunn (<a href="mailto:david.c.hunn@gmail.com?subject=Analyzing Streaming Data Using R">david.c.hunn@gmail.com</a>), Consultant, Alacer</div><div class="author" xmlns="">Howard A. Hoffer (<a href="mailto:aaron.hoffer@alacergroup.com?subject=Analyzing Streaming Data Using R">aaron.hoffer@alacergroup.com</a>), Senior Solution Architect, Alacer Technology Solutions, Alacer Group</div><div class="author" xmlns="">Ed Sarausad (<a href="mailto:ed@alacergroup.com?subject=Analyzing Streaming Data Using R">ed@alacergroup.com</a>), Managing Partner, Alacer Technology Solutions, Alacer Group</div><p></p><p><strong>Summary:</strong>  Web server logs can be used to study the browsing habits of users. For example, in [1], the authors use mixtures of first-order Markov models to discover clusters of Web pages on a web site. They use these models to predict which pages users are likely to request next. The authors propose using this information to “pre-fetch” a page before a user actually requests it. In this work, we perform a similar analysis of web server logs to predict how users browse a website. We perform the analysis in real-time using IBM Infosphere Streams combined with R. To perform the stream clustering we use a recent implementation of the extensible Markov model called rEMM [2].</p><p class="ibm-no-print"><a href="http://www.ibm.com/developerworks/views/java/libraryview.jsp?search_by=taming+tiger:">View more content in this series</a></p><p class="ibm-no-print"><div id="dw-tag-this" class="ibm-no-print"><a class="ibm-external-link" onclick="jQuery.launchTagThisWindow(); return false;" href="#">Tag this!</a></div></p></div><div class="ibm-column ibm-second"><p class="leading"><strong xmlns="">Date:</strong>&nbsp; 01 Dec 2010<br /><strong>Level: </strong>&nbsp;Intermediate<br class="ibm-ind-link" xmlns="" /><strong xmlns="">PDF:</strong>&nbsp; <a href="http://public.dhe.ibm.com/software/dw/java/template-dw-article-6.0-pdf.pdf" xmlns="">A4 and Letter</a> (56KB | 8 pages)<a href="http://www.adobe.com/products/acrobat/readstep2.html" class="ibm-external-link" xmlns="">Get Adobe&#174; Reader&#174;</a><br /><strong>Comments:</strong>   <span id="nCmts"><img alt="" src="http://dw1.s81c.com/developerworks/i/circle-preloader.gif" height="12" width="50" /><img alt="" src="http://dw1.s81c.com/i/gif" height="14" width="1" /></span><!-- Rating_Area_Begin --><!-- Ensure that div id is based on input id and ends with -widget --><input id="art-rating" name="ratinga" type="hidden" value="0" /><div id="art-rating-widget"></div><script language="JavaScript" type="text/javascript">
// 
   // widget div id and article id as args
   window.artRating.init('art-rating-widget');
// 
</script><!-- Rating_Area_End --></p></div></div></div><!-- dW_Summary_Area_END --><!-- CONTENT_BODY --><div id="ibm-content-body"><!-- MAIN_COLUMN_BEGIN --><div id="ibm-content-main"><!-- Related_Searches_Area_Begin --><script type="text/javascript" language="javascript">
	     capture_referrer();
</script><div id="dw-related-searches-article" style="display:none"><div class="ibm-container ibm-alternate-two"><div class="ibm-container-body"><!--  START : HTML FOR ARTICLE SEARCH --><div id="article_results" style="display:block"></div><!--  END : HTML FOR ARTICLE SEARCH --></div></div></div><!-- Related_Searches_Area_End --><!-- MAIN_COLUMN_CONTAINER_BEGIN --><div class="ibm-container"><!-- MAIN_COLUMN_CONTENT_BEGIN --><p xmlns=""><a name="overview"><span class="atitle">Overview</span></a></p><ul xmlns=""><li>Benefits of streams/R integration</li><li>The Problem: Use web server logs to predict visitors' next content request</li><li>The Solution: Use EMM</li><li>Details of solution</li><li>Results</li><li>Conclusions</li><li>Opportunities for follow-on work</li></ul><p xmlns=""><a name="IDAUFRVB"><span class="atitle">Introduction</span></a></p><p xmlns="">
            	The Extensible Markov Model (EMM), first introduced in <a href="#resources">2004 by Dunham, Meng, and Huang</a>, combines a stream clustering algorithm with a Markov chain. The states of the Markov chain are clusters defined by the stream clustering algorithm. The transition probabilities between states define a temporal component of the model. The EMM is able to change over time by both adding new states as they are encountered and fading or trimming existing states over time. This allows the model to adapt to changes in the simulated system. This could be very useful in the context of modeling traffic on a web site using web server using log files. The web site is likely to exhibit dynamic usage patterns as well as structural changes over time. Thus, a model that can account for these temporal changes in state and usage would seemingly be advantageous. The EMM promises this possibility.
            </p><p xmlns="">
            	In this paper we demonstrate the use of an EMM in modeling and predicting user requests for content on a web site. We use server logs to both train and test the model. The solution is implemented using IBM InfoSphere Streams and R. The application framework is provided by Streams and R is used modeling and prediction.
            </p><div class="ibm-alternate-rule" xmlns=""><hr /></div><p class="ibm-ind-link ibm-back-to-top" xmlns=""><a class="ibm-anchor-up-link" href="#ibm-pcon">Back to top</a></p><p xmlns=""><a name="IDAOGRVB"><span class="atitle">Benefits of Streams/R Integration</span></a></p><p xmlns="">Aaron, I could use your help in this section.</p><div class="ibm-alternate-rule" xmlns=""><hr /></div><p class="ibm-ind-link ibm-back-to-top" xmlns=""><a class="ibm-anchor-up-link" href="#ibm-pcon">Back to top</a></p><p xmlns=""><a name="IDA2GRVB"><span class="atitle">Predicting Content Requests from Web Server Logs</span></a></p><p xmlns="">The EMM model has been used to in traffic analysis, more examples ... In this work, we demonstrate the use of an EMM in predicting user requests for content on a website. </p><p xmlns=""><a name="IDAIHRVB"><span class="smalltitle">Web Server Logs</span></a></p><p xmlns=""> 
            	Web servers maintain a log of page requests. Among the items recorded for each request, the server logs the IP address of the requester, the time stamp of the request, and a path to the requested item. <a href="#table1">Table 1</a> below shows three example entries from a web server log. All three entries are from the same IP address for three separate items (two images and an HTML document). Note that server logs typically include two additional columns: a referrer URL and user agent. These three entries could represent a visit (or part thereof) to a web site. 
            </p><br /><a name="table1" xmlns=""><strong>Table 1. Example entries in a web server log</strong></a><br /><table border="0" cellpadding="0" cellspacing="0" class="ibm-data-table" summary="Example entries in a web server log" xmlns=""><tr><th scope="col">IP Address</th><th scope="col">User name etc</th><th scope="col">Time stamp</th><th scope="col">Access Request</th><th scope="col">Result Status Code</th><th scope="col">Bytes Transferred</th></tr><tr><td>123.123.123.123</td><td>- -</td><td>[26/Apr/2000:00:23:45 -0400]</td><td>"GET /pics/wpaper.gif HTTP/1.0"</td><td>200</td><td>6248</td></tr><tr><td>123.123.123.123</td><td>- -</td><td>[26/Apr/2000:00:23:48 -0400]</td><td>"GET /index.html HTTP/1.0"</td><td>200</td><td>8130</td></tr><tr><td>123.123.123.123</td><td>- -</td><td>[26/Apr/2000:00:23:50 -0400]</td><td>"GET /pics/5star2000.gif HTTP/1.0"</td><td>200</td><td>4005</td></tr></table><p xmlns=""><a name="IDASLRVB"><span class="smalltitle">The Dataset</span></a></p><p xmlns="">
            	For this exercise, we analyze Web server logs for <a href="http://ita.ee.lbl.gov/html/contrib/ClarkNet-HTTP.html">ClarkNet</a>—an Internet access provider for the Metro Baltimore-Washington DC area. The log contains approximately 1.6 million entries spanning a seven day period from midnight August 28, 1995 through midnight of September 3 1995. 
            </p><p xmlns="">
            	We limit our interest to requests for HTML pages, PDF documents, and PostScript documents. Thus, we ignore all requests for images (e.g. JPG, GIF), JavaScript, or other non-content items. We only include entries that result in successful requests, that is log entries that have results status code 200. We also attempt to filter out requests made by bots or spiders. To identify requests originating from bots we examine every entry in the log and compile a list of IP addresses who access <code>robots.txt</code>. This is likely not sufficient for a production solution, but for our purposes it likely sufficed. After applying the above filtering criteria, the original 1.6 million lines were reduced to approximate 350 thousand.
            </p><p xmlns="">
            	The remaining entries are ordered by IP address and then by time stamp. This sorting is necessary, because web servers log requests in the order they are received. Since, most web sites will have multiple simultaneous visitors, the requests for any particular user will be mixed up with other users' requests. Sorting by IP address and then by time stamp recreates the series of requests made by each user of the site. Thus sorted, the entries are split into visits. In this case, we define a visit as a series of GET requests no two of which are greater than 15 minutes apart. Once we have the sorted visits, we have a suitable dataset for training the EMM.
            </p><p xmlns="">
            	We use a simple single order Markov Model where each page (HTML, PDF, PS) is a node in the graph. We use one additional node to represent a user leaving the site.
            </p><div class="ibm-alternate-rule" xmlns=""><hr /></div><p class="ibm-ind-link ibm-back-to-top" xmlns=""><a class="ibm-anchor-up-link" href="#ibm-pcon">Back to top</a></p><p xmlns=""><a name="IDAYMRVB"><span class="atitle">Details of the Solution</span></a></p><p xmlns="">
            	For this project, we were interested in modeling a web site using a server log file. For our relatively simple model, each requested content item is a state or cluster in the EMM. The transition probabilities are determined by observing the number of people who request page i after requesting page j. In addition to modeling each content item, we also include a state that represents off-site. That is the state prior to a user requesting any content from the web server.
            </p><p xmlns=""><a href="#listing1">Code listing 1</a> below is the main SPL code for our application. The application is composed of five operators. The FileSource operator reads the input server log one line at time and returns it as a tuple with one item. Each line is parsed using an RScript operator that calls an R script. The R script parses out the IP address of the requester and the requested item. This script also analyzes the log entry to determine if it is relevant (per the conditions previously mentioned). The next operator in the pipe is a Filter that will filter any tuples tagged for filtration by the R parsing routine. Tuples that are not filtered are passed to the second RScript operator which passes the path of the requested item to an R script that uses the path to produce a prediction consisting of a list of the twenty most likely content items to be requested next. The resulting prediction is packaged up in a tuple along with the requesting IP address and the requested content item. The final operator, a FileSink operator, writes incoming tuples to an output file.

            </p><br /><a name="listing1" xmlns=""><strong>Listing 1. SPL code defining the application graph.</strong></a><br /><table border="0" cellspacing="0" summary="This table contains a code listing." cellpadding="0" width="100%" xmlns=""><tr><td class="code-outline"><pre class="displaycode">
composite Main {
  graph
    // read entries from the log file
    stream&lt;rstring line&gt; 
    inStream = FileSource() {
      param  file : "../data/first-100-test-log.txt";
    } 
 
    // parse the line using an R script
    stream&lt;rstring origin, rstring path&gt; 
    parsedStream = RScript(inStream) {
      param
        initializationScriptFileName : "../rsrc/parse-functions.R";
        rScriptFileName : "../rsrc/parse.R";
        streamAttributes : line;
        rObjects : "log.entry"; 

      output
        parsedStream: origin = fromR("origin"),
                      path = fromR("path");
    }

    // filter out invalid tuples:
    stream&lt;rstring origin, rstring path&gt;
    filteredStream = Filter(parsedStream) {
      param filter : origin != "filter me";
    }

    // call the r script that will create the prediction
    stream&lt;rstring origin, rstring path, list&lt;rstring&gt; prediction&gt; 
    analyzedStream = RScript(filteredStream) {
      param
        initializationScriptFileName : "../rsrc/init_predict.R";
        rScriptFileName : "../rsrc/predict.R";
        streamAttributes : path;
        rObjects : "path";
          
      output
        analyzedStream:
          prediction = fromR("prediction");
    }

    // send the results to a file
    () as dbg = FileSink(analyzedStream) {
      param
        format : csv;
        file   : "result.txt" ;   
    }
}
</pre></td></tr></table><br /><p xmlns=""><a href="listing2">Code listing 2</a> below contains the R functions used in parsing log entries. Note, the application maintains a list of known bot IP addresses. This list is used to label entries associated with bots for filtration. In addition, the script identifies requests for items other than HTML, PDF, or PS documents and marks the returned tuple for filtration.
</p><br /><a name="listing2" xmlns=""><strong>Listing 2. R functions used in parsing log entries.</strong></a><br /><table border="0" cellspacing="0" summary="This table contains a code listing." cellpadding="0" width="100%" xmlns=""><tr><td class="code-outline"><pre class="displaycode">
# This script contains functions used to parse a line from
# a web server log.

if (file.exists("../data/bot-list.RData")) {
  load("../data/bot-list.RData")  
} else {
  bot.list &lt;- c("")
}

origin.pattern &lt;- "^\\S+(\\.\\S+)*\\.\\S+"
path.pattern &lt;- "GET\\s+[/\\w]+.\\w+"

get.code &lt;- function(log.entry) {
  # Returns the return code of a web server log entry.
  tokens = unlist(strsplit(x=log.entry, split="\\s+", perl=TRUE))
  code = tokens[length(tokens)-1]
  
  return(code)
}

get.path &lt;- function(log.entry) {
  # Returns the path of the requested resource of a web server log entry.
  path &lt;- regmatches(x=log.entry, m=regexpr(pattern=path.pattern, 
                                               text=log.entry, 
                                               perl=TRUE))
  path &lt;- unlist(strsplit(x=path, split="\\s+", perl=TRUE))[2]
  
  return(path)
}

get.origin &lt;- function(log.entry) {
  # Returns the originating ip or domain from a web server log entry.
  return(regmatches(x=log.entry, m=regexpr(pattern=origin.pattern, 
                                           text=log.entry, 
                                           perl=TRUE)))
}

get.tuple &lt;- function(log.entry) {
  # Returns a two-item list containing the modified origin of the 
  # request and the path to the requested item.
  # The origin will be set to "filter me" if:
  #   -- The return code is not 200
  #   -- The requested file is not HTML, PDF, or PS
  #   -- The origin is determined to be a bot
  origin &lt;- get.origin(log.entry)
  path &lt;- get.path(log.entry)
  code &lt;- get.code(log.entry)
  
  if (code != "200") {
    origin &lt;- "filter me"
  } else if(is.null(path)) {
    origin &lt;- "filter me"
  } else if(path == FALSE) {
    origin &lt;- "filter me"
  } else if (! grepl(pattern="\\S+\\.(htm|html|pdf|ps)$", x=path, perl=TRUE)) {
    origin &lt;- "filter me"
  } else if (origin %in% bot.list) {
    origin &lt;- "filter me"
  }

  return(list(origin, path))
}
</pre></td></tr></table><br /><br /><a name="listing3" xmlns=""><strong>Listing 3. R script called to parse a log entry tuple.</strong></a><br /><table border="0" cellspacing="0" summary="This table contains a code listing." cellpadding="0" width="100%" xmlns=""><tr><td class="code-outline"><pre class="displaycode">
# This script parses log entries transforming them into tuples
# It parses out the origin of the request and the requested path.
# 
# inputs:  log.entry -- A single line from a web server log.
# outputs: origin -- The origin of the request received by web 
#                    server.
#          path -- The path to the requested item.
#
# The origin will be set to "filter me" if:
#   -- The return code is not 200
#   -- The requested file is not HTML, PDF, or PS
#   -- The origin is determined to be a bot

tuple &lt;- unlist(get.tuple(log.entry))
origin &lt;- tuple[1]
path &lt;- tuple[2]
</pre></td></tr></table><br /><br /><a name="listing4" xmlns=""><strong>Listing 4. R script for initializing the EMM model.</strong></a><br /><table border="0" cellspacing="0" summary="This table contains a code listing." cellpadding="0" width="100%" xmlns=""><tr><td class="code-outline"><pre class="displaycode">
# Load required libraries
library(rEMM)
library(hash)

# Load the Model
stopifnot(file.exists("../data/emm.RData"))
load(file="../data/emm.RData")

# Load the pages hash table
stopifnot(file.exists("../data/page-hash.RData"))
load(file="../data/page-hash.RData")

# Load the pages list
stopifnot(file.exists("../data/page-list.RData"))
load(file="../data/page-list.RData")
</pre></td></tr></table><br /><br /><a name="listing5" xmlns=""><strong>Listing 5. R script for making predictions using the EMM.</strong></a><br /><table border="0" cellspacing="0" summary="This table contains a code listing." cellpadding="0" width="100%" xmlns=""><tr><td class="code-outline"><pre class="displaycode">
# inputs:  path -- The path of the last reqested item.
# outputs: prediction -- A list of the twenty most likely next paths to be 
#                        reqested.

prediction &lt;- vector()
last.request &lt;- page.hash[[path]]
if (! is.null(last.request)) {
  liklihoods &lt;- predict(emm, current_state=last.request, 
                           probabilities=TRUE)
  top.twenty &lt;- names(sort(x=liklihoods, decreasing=TRUE)[1:20])
  for (i in 1:20) {
    prediction[[i]] &lt;- toString(page.list[as.numeric(top.twenty[i])])
  } 
}
</pre></td></tr></table><br /><!-- CMA ID: 0 --> <!-- Site ID: 1 --> <!-- XSLT stylesheet used to transform this file: dw-document-html-6.0.xsl --> <br /><div class="ibm-alternate-rule" xmlns=""><hr /></div><p class="ibm-ind-link ibm-back-to-top" xmlns=""><a class="ibm-anchor-up-link" href="#ibm-pcon">Back to top</a></p><p xmlns=""><span class="atitle"><a name="download">Downloads</a></span></p><table border="0" cellpadding="0" cellspacing="0" class="ibm-data-table" summary="This table contains downloads for this document." width="100%" xmlns=""><tr><th scope="col">Description</th><th scope="col">Name</th><th scope="col">Size</th><th scope="col">Download method</th></tr><tr><td scope="row" class="tb-row">Sample Perl scripts for this tutorial</td><td nowrap="nowrap">DoNotLeaveThisLink.zip</td><td nowrap="nowrap">10KB</td><td nowrap="nowrap"><a class="fbox" href="http://www.ibm.com/developerworks/apps/download/index.jsp?contentid=0&amp;filename=DoNotLeaveThisLink.zip&amp;method=http&amp;locale=">HTTP</a></td></tr><tr><td scope="row" class="tb-row">A related PDF (not of the tutorial)<sup>1</sup></td><td nowrap="nowrap">DoNotLeaveThisLink.pdf</td><td nowrap="nowrap">50KB</td><td nowrap="nowrap"><a class="fbox" href="http://public.dhe.ibm.com/software/dw/xxx/DoNotLeaveThisLink.pdf">HTTP</a></td></tr></table><p xmlns=""><a class="ibm-forward-link" href="/developerworks/library/whichmethod.html">Information about download methods</a>          <a class="ibm-external-link" href="http://www.adobe.com/products/acrobat/readstep2.html">Get Adobe&#174; Reader&#174;</a></p><p xmlns=""><strong>More downloads</strong></p><ul xmlns=""><li>Demo: <a href="http://demo-page.html">How to code a widget</a></li><li>Presentation: <a href="http://prez-page.html">Why code widgets instead of whatnots</a><sup>2</sup></li></ul><p xmlns=""><strong>Notes</strong></p><ol><li xmlns="">This is a sample note about the PDF.</li><li xmlns="">This is a sample note about the presentation.</li></ol><br /><p xmlns=""><a name="resources"><span class="atitle">Resources</span></a></p><ul xmlns=""><li><a href="http://www.ibm.com/developerworks/forums/dw_forum.jsp?forum=375&amp;cat=5">Participate in the discussion forum</a>.<br /><br /></li><li><a href="http://www.jstatsoft.org/v35/i05/paper">
                    M. Hahsler and M. H. Dunham, “rEMM: Extensible Markov Model for Data Stream Clustering in R,” Journal of Statistical Software, vol. 35, no. 5, pp. 1–31, 2010.
                </a><br /><br /></li><li><a href="http://www.jstor.org/discover/10.2307/1391073?uid=3739960&amp;uid=2129&amp;uid=2&amp;uid=70&amp;uid=4&amp;uid=3739256&amp;sid=21103603113867">
                    R. Sen and M. H. Hansen, “Predicting Web Users’ Next Access Based on Log Data,” Journal of Computational and Graphical Statistics, vol. 12, no. 1, pp. 143–155, Mar. 2003.
                </a><br /><br /></li><li><a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=1410313&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1410313">
                    M. H. Dunham, Y. Meng, and J. Huang, “Extensible Markov Model,” in Data Mining, 2004. ICDM’04. Fourth IEEE International Conference on, 2004, pp. 371–374.
                </a><br /><br /></li></ul><p xmlns=""><a name="author"><span class="atitle">About the authors</span></a></p><div class="ibm-container ibm-portrait-module ibm-alternate-two" xmlns=""></div><!-- MAIN_COLUMN_CONTENT_END --><!-- INLINE_COMMENTS_START --><p class="ibm-no-print"><span class="atitle"><a name="icomments">Comments</a></span></p><div id="dw-icomments-container" class="ibm-no-print"><div class="ibm-alternate-rule"><hr /></div><div class="ibm-alternate-rule"><hr /></div><!-- Comment_Script --><a id="comments" href="comments"></a><div id="threadShow"></div><script language="JavaScript" type="text/javascript">
// 
jQuery('threadShow').insertComment('95%',5,'nCmts','icomments');
// 
</script></div><!-- INLINE_COMMENTS_END --><p class="ibm-ind-link ibm-back-to-top"><a class="ibm-anchor-up-link" href="#ibm-pcon">Back to top</a></p><p><a href="http://www.ibm.com/developerworks/ibm/trademarks/">Trademarks</a></p></div><!-- MAIN_COLUMN_CONTAINER_END --><!-- Rating_Meta_BEGIN --><!--Rating_Meta_BEGIN--><div class="metavalue" xmlns="">static.content.url=http://www.ibm.com/developerworks/js/artrating/</div><div class="metavalue" xmlns="">SITE_ID=1</div><div class="metavalue" xmlns="">Zone=Java technology, Open source</div><div class="metavalue" xmlns="">ArticleID=0</div><div class="metavalue" xmlns="">ArticleTitle=R and Streams: Analyzing Streaming Data Using R</div><div class="metavalue" xmlns="">publish-date=12012010</div><script language="javascript" type="text/javascript">document.write('<div class="metavalue">url='+location.href.replace(/</g,  '%3C')+'</div>');</script><!--Rating_Meta_END--><!-- Rating_Meta_END --></div><!-- MAIN_COLUMN_END--><!-- RIGHT_COLUMN_BEGIN --><div id="ibm-content-sidebar"><!-- RIGHT_COLUMN_CONTENT_BEGIN --><div class="ibm-container" xmlns=""><h2>Table of contents</h2><div class="ibm-container-body"><img src="file:///C:/Users/Dave/Documents/Workspace/alacer/emm/developerworks/web/www.ibm.com/i/c.gif" width="1" height="1" alt="" /><ul class="ibm-bullet-list"><li><a href="#overview" class="ibm-feature-link">Overview</a></li><li><a href="#IDAUFRVB" class="ibm-feature-link">Introduction</a></li><li><a href="#IDAOGRVB" class="ibm-feature-link">Benefits of Streams/R Integration</a></li><li><a href="#IDA2GRVB" class="ibm-feature-link">Predicting Content Requests from Web Server Logs</a></li><li><a href="#IDAYMRVB" class="ibm-feature-link">Details of the Solution</a></li><li><a href="#download" class="ibm-feature-link">Downloads</a></li><li><a href="#resources" class="ibm-feature-link">Resources</a></li><li><a href="#icomments" class="ibm-feature-link">Comments</a></li></ul></div></div><!-- Dig_Deeper --><!-- High_Visibility_Offer --><!-- Special_Offers --><!-- RIGHT_COLUMN_CONTENT_END --></div><!-- RIGHT_COLUMN_END --><!-- CONTENT_BODY_END --></div></div><!-- CONTENT_END --><!-- END_IBM-PCON --></div><!-- FOOTER_BEGIN --><div id="ibm-footer" /><div id="ibm-page-tools-dw"><div id="dw-footer-top-row" class="dw-mf-minimal"></div></div><div id="ibm-footer-module-dw" class="dw-mf-minimal"></div><!-- FOOTER_END --><!-- END_IBM-TOP --></div><!-- SCRIPTS_INCLUDE_BEGIN --><!-- JQuery start --><script type="text/javascript" language="JavaScript" src="http://dw1.s81c.com/developerworks/js/jquery/cluetip98/jquery.dimensions-1.2.js">//</script><script type="text/javascript" language="JavaScript" src="http://dw1.s81c.com/developerworks/js/jquery/cluetip98/jquery.hoverIntent.minified.js">//</script><script type="text/javascript" language="JavaScript" src="http://dw1.s81c.com/developerworks/js/jquery/cluetip98/jquery.cluetip.js">//</script><script type="text/javascript" language="JavaScript">
	jQuery.noConflict();     
	// Put all your code in your document ready area
	jQuery(document).ready(function(jQuery) {
	// Do jQuery stuff using jQuery 
	jQuery('a.dwauthor').cluetip({
		local: true,
		showTitle: false,
		positionBy: 'bottomTop',
		sticky: true,	
		mouseOutClose: true,
		closeText: '<img src="http://dw1.s81c.com/developerworks/js/jquery/cluetip98/i/x.gif" alt="Close" />',
		arrows: false,
		dropShadow: true,
		cluetipClass: 'dwbasic'
		});   	
	});
 </script><!-- JQuery end --><div id="ibm-metrics" /></body></html>